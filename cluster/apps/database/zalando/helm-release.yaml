---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2beta1.json
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: &app postgres-operator
  namespace: database
spec:
  interval: 30m
  chart:
    spec:
      chart: postgres-operator
      version: 1.10.1
      sourceRef:
        kind: HelmRepository
        name: zalando-postgres-operator-charts
        namespace: flux-system
  install:
    createNamespace: true
    remediation:
      retries: 5
  upgrade:
    remediation:
      retries: 5
  values:

    # general configuration parameters
    configGeneral:
      docker_image: ghcr.io/maksimkurb/spilo-15:3.1-p1-pgvecto.rs-3

      # min number of instances in Postgres cluster. -1 = no limit
      min_instances: -1
      # max number of instances in Postgres cluster. -1 = no limit
      max_instances: 2
      # period between consecutive repair requests
      repair_period: 5m
      # period between consecutive sync requests
      resync_period: 30m
      # can prevent certain cases of memory overcommitment
      # set_memory_request_to_limit: false

      # map of sidecar names to docker images
      # sidecar_docker_images:
      #  example: "exampleimage:exampletag"

      # number of routines the operator spawns to process requests concurrently
      workers: 8

    configMajorVersionUpgrade:
      major_version_upgrade_mode: "manual"

      # minimal Postgres major version that will not automatically be upgraded
      minimal_major_version: "11"
      # target Postgres major version when upgrading clusters automatically
      target_major_version: "15"

    configKubernetes:
      # list of additional capabilities for postgres container
      # additional_pod_capabilities:
      # - "SYS_NICE"

      enable_readiness_probe: true

      cluster_domain: cluster.local
      cluster_labels:
        application: spilo
      cluster_name_label: cluster-name

      # allow user secrets in other namespaces than the Postgres cluster
      enable_cross_namespace_secret: true
      enable_pod_antiaffinity: true
      enable_pod_disruption_budget: true

      # annotations to be ignored when comparing statefulsets, services etc.
      # ignored_annotations:
      # - k8s.v1.cni.cncf.io/network-status

      # switches pod anti affinity type to `preferredDuringSchedulingIgnoredDuringExecution`
      pod_antiaffinity_preferred_during_scheduling: false
      # override topology key for pod anti affinity
      pod_antiaffinity_topology_key: "kubernetes.io/hostname"
      # namespaced name of the ConfigMap with environment variables to populate on every pod
      pod_environment_configmap: "database/zalando-env"
      # name of the Secret (in cluster namespace) with environment variables to populate on every pod
      pod_environment_secret: "zalando-s3"

      # specify the pod management policy of stateful sets of Postgres clusters
      pod_management_policy: "ordered_ready"
      # label assigned to the Postgres pods (and services/endpoints)
      pod_role_label: spilo-role
      # service account definition as JSON/YAML string to be used by postgres cluster pods
      # pod_service_account_definition: ""

      # role binding definition as JSON/YAML string to be used by pod service account
      # pod_service_account_role_binding_definition: ""

      # Postgres pods are terminated forcefully after this timeout
      pod_terminate_grace_period: 5m
      # template for database user secrets generated by the operator,
      # here username contains the namespace in the format namespace.username
      # if the user is in different namespace than cluster and cross namespace secrets
      # are enabled via `enable_cross_namespace_secret` flag in the configuration.
      secret_name_template: "{username}.{cluster}.credentials.{tprkind}.{tprgroup}"
      # sharing unix socket of PostgreSQL (`pg_socket`) with the sidecars
      share_pgsocket_with_sidecars: false
      # set user and group for the spilo container (required to run Spilo as non-root process)
      spilo_runasuser: 101
      spilo_runasgroup: 103

      # group ID with write-access to volumes (required to run Spilo as non-root process)
      spilo_fsgroup: 103

      # whether the Spilo container should run in privileged mode
      spilo_privileged: false
      # whether the Spilo container should run with additional permissions other than parent.
      # required by cron which needs setuid
      spilo_allow_privilege_escalation: true
      # storage resize strategy, available options are: ebs, pvc, off or mixed
      storage_resize_mode: off # We do not need to increase PVC size as it is NFS anyways

      # operator watches for postgres objects in the given namespace
      watched_namespace: "*"  # listen to all namespaces

    # configure resource requests for the Postgres pods
    configPostgresPodResources:
      # CPU limits for the postgres containers
      default_cpu_limit: "1"
      # CPU request value for the postgres containers
      default_cpu_request: 100m
      # memory limits for the postgres containers
      default_memory_limit: 500Mi
      # memory request value for the postgres containers
      default_memory_request: 100Mi
      # optional upper boundary for CPU request
      # max_cpu_request: "1"

      # optional upper boundary for memory request
      # max_memory_request: 4Gi

      # hard CPU minimum required to properly run a Postgres cluster
      min_cpu_limit: 250m
      # hard memory minimum required to properly run a Postgres cluster
      min_memory_limit: 250Mi

    # timeouts related to some operator actions
    configTimeouts:
      # interval between consecutive attempts of operator calling the Patroni API
      patroni_api_check_interval: 1s
      # timeout when waiting for successful response from Patroni API
      patroni_api_check_timeout: 5s
      # timeout when waiting for the Postgres pods to be deleted
      pod_deletion_wait_timeout: 10m
      # timeout when waiting for pod role and cluster labels
      pod_label_wait_timeout: 10m
      # interval between consecutive attempts waiting for postgresql CRD to be created
      ready_wait_interval: 3s
      # timeout for the complete postgres CRD creation
      ready_wait_timeout: 30s
      # interval to wait between consecutive attempts to check for some K8s resources
      resource_check_interval: 3s
      # timeout when waiting for the presence of a certain K8s resource (e.g. Sts, PDB)
      resource_check_timeout: 10m

    # configure behavior of load balancers
    configLoadBalancer:
      # DNS zone for cluster DNS name when load balancer is configured for cluster
      db_hosted_zone: pg.cubly.ru
      # annotations to apply to service when load balancing is enabled
      # custom_service_annotations:
      #   keyx: valuez
      #   keya: valuea

      # toggles service type load balancer pointing to the master pod of the cluster
      enable_master_load_balancer: true
      # toggles service type load balancer pointing to the master pooler pod of the cluster
      enable_master_pooler_load_balancer: false
      # toggles service type load balancer pointing to the replica pod of the cluster
      enable_replica_load_balancer: false
      # toggles service type load balancer pointing to the replica pooler pod of the cluster
      enable_replica_pooler_load_balancer: false
      # define external traffic policy for the load balancer
      external_traffic_policy: "Cluster"
      # defines the DNS name string template for the master load balancer cluster
      master_dns_name_format: "{cluster}.{namespace}.{hostedzone}"
      # deprecated DNS template for master load balancer using team name
      master_legacy_dns_name_format: "{cluster}.{team}.{hostedzone}"
      # defines the DNS name string template for the replica load balancer cluster
      replica_dns_name_format: "{cluster}-repl.{namespace}.{hostedzone}"
      # deprecated DNS template for replica load balancer using team name
      replica_legacy_dns_name_format: "{cluster}-repl.{team}.{hostedzone}"

    # options to aid debugging of the operator itself
    configDebug:
      # toggles verbose debug logs from the operator
      debug_logging: true
      # toggles operator functionality that require access to the postgres database
      enable_database_access: true

    # parameters affecting logging and REST API listener
    configLoggingRestApi:
      # REST API listener listens to this port
      api_port: 8080
      # number of entries in the cluster history ring buffer
      cluster_history_entries: 1000
      # number of lines in the ring buffer used to store cluster logs
      ring_log_lines: 100

    # configure interaction with non-Kubernetes objects from AWS or GCP
    configAwsOrGcp:
      # Additional Secret (aws or gcp credentials) to mount in the pod
      # additional_secret_mount: "some-secret-name"

      # Path to mount the above Secret in the filesystem of the container(s)
      # additional_secret_mount_path: "/some/dir"

      # AWS region used to store EBS volumes
      aws_region: default

      # enable automatic migration on AWS from gp2 to gp3 volumes
      enable_ebs_gp3_migration: false
      # defines maximum volume size in GB until which auto migration happens
      # enable_ebs_gp3_migration_max_size: 1000

      # GCP credentials that will be used by the operator / pods
      # gcp_credentials: ""

      # AWS IAM role to supply in the iam.amazonaws.com/role annotation of Postgres pods
      # kube_iam_role: ""

      # S3 bucket to use for shipping postgres daily logs
      log_s3_bucket: "postgres-logs"

      # S3 bucket to use for shipping WAL segments with WAL-E
      wal_s3_bucket: "postgres-wal"

      # GCS bucket to use for shipping WAL segments with WAL-E
      # wal_gs_bucket: ""

      # Azure Storage Account to use for shipping WAL segments with WAL-G
      # wal_az_storage_account: ""

    # configure K8s cron job managed by the operator
    configLogicalBackup:
      # Azure Storage Account specs to store backup results
      # logical_backup_azure_storage_account_name: ""
      # logical_backup_azure_storage_container: ""
      # logical_backup_azure_storage_account_key: ""

      # resources for logical backup pod, if empty configPostgresPodResources will be used
      # logical_backup_cpu_limit: ""
      # logical_backup_cpu_request: ""
      # logical_backup_memory_limit: ""
      # logical_backup_memory_request: ""

      # image for pods of the logical backup job (example runs pg_dumpall)
      logical_backup_docker_image: "registry.opensource.zalan.do/acid/logical-backup:v1.10.1"
      # path of google cloud service account json file
      # logical_backup_google_application_credentials: ""

      # prefix for the backup job name
      logical_backup_job_prefix: "logical-backup-"
      # storage provider - either "s3", "gcs" or "az"
      logical_backup_provider: "s3"
      # S3 Access Key ID
      logical_backup_s3_access_key_id: "${SECRET_MINIO_ACCESS_KEY}"
      # S3 bucket to store backup results
      logical_backup_s3_bucket: "my-bucket-url"
      # S3 region of bucket
      logical_backup_s3_region: "default"
      # S3 endpoint url when not using AWS
      logical_backup_s3_endpoint: "http://minio.default:9000"
      # S3 Secret Access Key
      logical_backup_s3_secret_access_key: "${SECRET_MINIO_SECRET_KEY}"
      # S3 server side encryption
      logical_backup_s3_sse: "AES256"
      # S3 retention time for stored backups for example "2 week" or "7 days"
      logical_backup_s3_retention_time: "3 month"
      # backup schedule in the cron format
      logical_backup_schedule: "30 00 * * *"

    # automate creation of human users with teams API service
    configTeamsApi:
      # team_admin_role will have the rights to grant roles coming from PG manifests
      enable_admin_role_for_users: true
      # operator watches for PostgresTeam CRs to assign additional teams and members to clusters
      enable_postgres_team_crd: false
      # toogle to create additional superuser teams from PostgresTeam CRs
      enable_postgres_team_crd_superusers: false
      # toggle to automatically rename roles of former team members and deny LOGIN
      enable_team_member_deprecation: false
      # toggle to grant superuser to team members created from the Teams API
      enable_team_superuser: false
      # toggles usage of the Teams API by the operator
      enable_teams_api: false
      # should contain a URL to use for authentication (username and token)
      # pam_configuration: https://info.example.com/oauth2/tokeninfo?access_token= uid realm=/employees

      # operator will add all team member roles to this group and add a pg_hba line
      pam_role_name: zalandos
      # List of teams which members need the superuser role in each Postgres cluster
      postgres_superuser_teams:
        - postgres_superusers
      # List of roles that cannot be overwritten by an application, team or infrastructure role
      protected_role_names:
        - admin
        - cron_admin
      # Suffix to add if members are removed from TeamsAPI or PostgresTeam CRD
      role_deletion_suffix: "_deleted"
      # role name to grant to team members created from the Teams API
      team_admin_role: admin
      # postgres config parameters to apply to each team member role
      team_api_role_configuration:
        log_statement: all
      # URL of the Teams API service
      # teams_api_url: http://fake-teams-api.default.svc.cluster.local

    # configure connection pooler deployment created by the operator
    configConnectionPooler:
      # db schema to install lookup function into
      connection_pooler_schema: "pooler"
      # db user for pooler to use
      connection_pooler_user: "pooler"
      # docker image
      connection_pooler_image: "registry.opensource.zalan.do/acid/pgbouncer:master-27"
      # max db connections the pooler should hold
      connection_pooler_max_db_connections: 60
      # default pooling mode
      connection_pooler_mode: "transaction"
      # number of pooler instances
      connection_pooler_number_of_instances: 2
      # default resources
      connection_pooler_default_cpu_request: 500m
      connection_pooler_default_memory_request: 100Mi
      connection_pooler_default_cpu_limit: "1"
      connection_pooler_default_memory_limit: 100Mi

    configPatroni:
      # enable Patroni DCS failsafe_mode feature
      enable_patroni_failsafe_mode: false

    # Zalando's internal CDC stream feature
    enableStreams: false

    rbac:
      # Specifies whether RBAC resources should be created
      create: true
      # Specifies whether ClusterRoles that are aggregated into the K8s default roles should be created. (https://kubernetes.io/docs/reference/access-authn-authz/rbac/#default-roles-and-role-bindings)
      createAggregateClusterRoles: false

    serviceAccount:
      # Specifies whether a ServiceAccount should be created
      create: true


    podServiceAccount:
      # The name of the ServiceAccount to be used by postgres cluster pods
      # If not set a name is generated using the fullname template and "-pod" suffix
      name: "postgres-pod"

    # priority class for operator pod
    priorityClassName: ""

    # priority class for database pods
    podPriorityClassName: ""

    resources:
      limits:
        cpu: 500m
        memory: 500Mi
      requests:
        cpu: 100m
        memory: 250Mi

    securityContext:
      runAsUser: 1000
      runAsNonRoot: true
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false

    # Allow to setup operator Deployment readiness probe
    readinessProbe:
      initialDelaySeconds: 5
      periodSeconds: 10

    # Affinity for pod assignment
    # Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    affinity: {}

    # Node labels for pod assignment
    # Ref: https://kubernetes.io/docs/user-guide/node-selection/
    nodeSelector: {}

    # Tolerations for pod assignment
    # Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []

    controllerID:
      # Specifies whether a controller ID should be defined for the operator
      # Note, all postgres manifest must then contain the following annotation to be found by this operator
      # "acid.zalan.do/controller": <controller-ID-of-the-operator>
      create: false
      # The name of the controller ID to use.
      # If not set and create is true, a name is generated using the fullname template
      name:
